{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "170431981-AIG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPPz4ARs9xfvg56UKEX6krN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TarekQMUL/AIG/blob/main/170431981_AIG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLFRoKQ3xeNI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f80fadd-d046-4067-82b3-83e076631d3d"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec 16 02:34:43 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ4i0Utsy8O-",
        "outputId": "1b95d719-0d99-43a0-af60-0a1b3a78ba18"
      },
      "source": [
        "!pip install baselines --no-deps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting baselines\n",
            "  Downloading baselines-0.1.5.tar.gz (123 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 33.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 33.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 30 kB 38.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 61 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 71 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 81 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 92 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 102 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 112 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 123 kB 15.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: baselines\n",
            "  Building wheel for baselines (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for baselines: filename=baselines-0.1.5-py3-none-any.whl size=163902 sha256=9c438a6d82af930aab80d568f5cff93b97504184339696d2cf1f6ea00f6acc43\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/27/40/3b266f230c825e6b219bfdd5447566d4d109b66fc4e0c074ed\n",
            "Successfully built baselines\n",
            "Installing collected packages: baselines\n",
            "Successfully installed baselines-0.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W427cc9-zNi9",
        "outputId": "59de16a0-6fd5-4281-b370-246fb74121e3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne4i-8qN0vHm",
        "outputId": "cec1fce1-5632-4631-87c5-c6f6576792a5"
      },
      "source": [
        "!python -m atari_py.import_roms /content/drive/MyDrive/game_ai/roms"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "copying adventure.bin from ROMS/Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
            "copying air_raid.bin from ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
            "copying alien.bin from ROMS/Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
            "copying amidar.bin from ROMS/Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
            "copying assault.bin from ROMS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
            "copying asterix.bin from ROMS/Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n",
            "copying asteroids.bin from ROMS/Asteroids (1981) (Atari, Brad Stewart - Sears) (CX2649 - 49-75163) [no copyright] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
            "copying atlantis.bin from ROMS/Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
            "copying bank_heist.bin from ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n",
            "copying battle_zone.bin from ROMS/Battlezone (1983) (Atari - GCC, Mike Feinstein, Brad Rice) (CX2681) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
            "copying beam_rider.bin from ROMS/Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
            "copying berzerk.bin from ROMS/Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
            "copying bowling.bin from ROMS/Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
            "copying boxing.bin from ROMS/Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
            "copying breakout.bin from ROMS/Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
            "copying carnival.bin from ROMS/Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
            "copying centipede.bin from ROMS/Centipede (1983) (Atari - GCC) (CX2676) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
            "copying chopper_command.bin from ROMS/Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
            "copying crazy_climber.bin from ROMS/Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
            "copying defender.bin from ROMS/Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
            "copying demon_attack.bin from ROMS/Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
            "copying donkey_kong.bin from ROMS/Donkey Kong (1982) (Coleco - Woodside Design Associates - Imaginative Systems Software, Garry Kitchen) (2451) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
            "copying double_dunk.bin from ROMS/Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
            "copying elevator_action.bin from ROMS/Elevator Action (1983) (Atari, Dan Hitchens) (CX26126) (Prototype) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
            "copying enduro.bin from ROMS/Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
            "copying fishing_derby.bin from ROMS/Fishing Derby (1980) (Activision, David Crane) (AG-004) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
            "copying freeway.bin from ROMS/Freeway (1981) (Activision, David Crane) (AG-009, AG-009-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
            "copying frogger.bin from ROMS/Frogger (1982) (Parker Brothers, Ed English, David Lamkins) (PB5300) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
            "copying frostbite.bin from ROMS/Frostbite (1983) (Activision, Steve Cartwright) (AX-031) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
            "copying galaxian.bin from ROMS/Galaxian (1983) (Atari - GCC, Mark Ackerman, Tom Calderwood, Glenn Parker) (CX2684) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
            "copying gopher.bin from ROMS/Gopher (Gopher Attack) (1982) (U.S. Games Corporation - JWDA, Sylvia Day, Todd Marshall, Robin McDaniel, Henry Will IV) (VC2001) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
            "copying gravitar.bin from ROMS/Gravitar (1983) (Atari, Dan Hitchens, Mimi Nyden) (CX2685) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
            "copying hero.bin from ROMS/H.E.R.O. (1984) (Activision, John Van Ryzin) (AZ-036-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
            "copying ice_hockey.bin from ROMS/Ice Hockey - Le Hockey Sur Glace (1981) (Activision, Alan Miller) (AX-012, CAX-012, AX-012-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
            "copying jamesbond.bin from ROMS/James Bond 007 (James Bond Agent 007) (1984) (Parker Brothers - On-Time Software, Joe Gaucher, Louis Marbel) (PB5110) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
            "copying journey_escape.bin from ROMS/Journey Escape (1983) (Data Age, J. Ray Dettling) (112-006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
            "copying kaboom.bin from ROMS/Kaboom! (Paddle) (1981) (Activision, Larry Kaplan, David Crane) (AG-010, AG-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
            "copying kangaroo.bin from ROMS/Kangaroo (1983) (Atari - GCC, Kevin Osborn) (CX2689) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
            "copying keystone_kapers.bin from ROMS/Keystone Kapers - Raueber und Gendarm (1983) (Activision, Garry Kitchen - Ariola) (EAX-025, EAX-025-04I - 711 025-725) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
            "copying king_kong.bin from ROMS/King Kong (1982) (Tigervision - Software Electronics Corporation, Karl T. Olinger - Teldec) (7-001 - 3.60001 VE) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
            "copying koolaid.bin from ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983) (M Network, Stephen Tatsumi, Jane Terjung - Kool Aid) (MT4648) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
            "copying krull.bin from ROMS/Krull (1983) (Atari, Jerome Domurat, Dave Staugas) (CX2682) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
            "copying kung_fu_master.bin from ROMS/Kung-Fu Master (1987) (Activision - Imagineering, Dan Kitchen, Garry Kitchen) (AG-039-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
            "copying laser_gates.bin from ROMS/Laser Gates (AKA Innerspace) (1983) (Imagic, Dan Oliver) (720118-2A, 13208, EIX-007-04I) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
            "copying lost_luggage.bin from ROMS/Lost Luggage (Airport Mayhem) (1982) (Apollo - Games by Apollo, Larry Minor, Ernie Runyon, Ed Salvo) (AP-2004) [no opening scene] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
            "copying montezuma_revenge.bin from ROMS/Montezuma's Revenge - Featuring Panama Joe (1984) (Parker Brothers - JWDA, Henry Will IV) (PB5760) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
            "copying mr_do.bin from ROMS/Mr. Do! (1983) (CBS Electronics, Ed English) (4L4478) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
            "copying ms_pacman.bin from ROMS/Ms. Pac-Man (1983) (Atari - GCC, Mark Ackerman, Glenn Parker) (CX2675) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
            "copying name_this_game.bin from ROMS/Name This Game (Guardians of Treasure) (1983) (U.S. Games Corporation - JWDA, Roger Booth, Sylvia Day, Ron Dubren, Todd Marshall, Robin McDaniel, Wes Trager, Henry Will IV) (VC1007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
            "copying pacman.bin from ROMS/Pac-Man (1982) (Atari, Tod Frye) (CX2646) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
            "copying phoenix.bin from ROMS/Phoenix (1983) (Atari - GCC, Mike Feinstein, John Mracek) (CX2673) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
            "copying video_pinball.bin from ROMS/Pinball (AKA Video Pinball) (Zellers).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
            "copying pitfall.bin from ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982) (Activision, David Crane) (AX-018, AX-018-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
            "copying pooyan.bin from ROMS/Pooyan (1983) (Konami) (RC 100-X 02) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
            "copying private_eye.bin from ROMS/Private Eye (1984) (Activision, Bob Whitehead) (AG-034-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
            "copying qbert.bin from ROMS/Q-bert (1983) (Parker Brothers - Western Technologies, Dave Hampton, Tom Sloper) (PB5360) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
            "copying riverraid.bin from ROMS/River Raid (1982) (Activision, Carol Shaw) (AX-020, AX-020-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
            "copying road_runner.bin from patched version of ROMS/Road Runner (1989) (Atari - Bobco, Robert C. Polaro) (CX2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
            "copying robotank.bin from ROMS/Robot Tank (Robotank) (1983) (Activision, Alan Miller) (AZ-028, AG-028-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
            "copying seaquest.bin from ROMS/Seaquest (1983) (Activision, Steve Cartwright) (AX-022) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
            "copying sir_lancelot.bin from ROMS/Sir Lancelot (1983) (Xonox - K-Tel Software - Product Guild, Anthony R. Henderson) (99006, 6220) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n",
            "copying skiing.bin from ROMS/Skiing - Le Ski (1980) (Activision, Bob Whitehead) (AG-005, CAG-005, AG-005-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
            "copying solaris.bin from ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986) (Atari, Douglas Neubauer, Mimi Nyden) (CX26136) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
            "copying space_invaders.bin from ROMS/Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
            "copying star_gunner.bin from ROMS/Stargunner (1983) (Telesys, Alex Leavens) (1005) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
            "copying surround.bin from ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
            "copying tennis.bin from ROMS/Tennis - Le Tennis (1981) (Activision, Alan Miller) (AG-007, CAG-007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
            "copying time_pilot.bin from ROMS/Time Pilot (1983) (Coleco - Woodside Design Associates, Harley H. Puthuff Jr.) (2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
            "copying trondead.bin from ROMS/TRON - Deadly Discs (TRON Joystick) (1983) (M Network - INTV - APh Technological Consulting, Jeff Ronne, Brett Stutz) (MT5662) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
            "copying tutankham.bin from ROMS/Tutankham (1983) (Parker Brothers, Dave Engman, Dawn Stockbridge) (PB5340) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
            "copying up_n_down.bin from ROMS/Up 'n Down (1984) (SEGA - Beck-Tech, Steve Beck, Phat Ho) (009-01) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
            "copying venture.bin from ROMS/Venture (1982) (Coleco, Joseph Biel) (2457) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
            "copying pong.bin from ROMS/Video Olympics - Pong Sports (Paddle) (1977) (Atari, Joe Decuir - Sears) (CX2621 - 99806, 6-99806, 49-75104) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
            "copying wizard_of_wor.bin from ROMS/Wizard of Wor (1982) (CBS Electronics - Roklan, Joe Hellesen, Joe Wagner) (M8774, M8794) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
            "copying yars_revenge.bin from ROMS/Yars' Revenge (Time Freeze) (1982) (Atari, Howard Scott Warshaw - Sears) (CX2655 - 49-75167) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
            "copying zaxxon.bin from ROMS/Zaxxon (1983) (Coleco) (2454) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GszC21Qp2ClH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df5f64d6-2119-4a34-fdc8-017775328543"
      },
      "source": [
        "\"\"\"\n",
        "Title: Deep Q-Learning for Atari Breakout\n",
        "Author: [Jacob Chapman](https://twitter.com/jacoblchapman) and [Mathias Lechner](https://twitter.com/MLech20)\n",
        "Date created: 2020/05/23\n",
        "Last modified: 2020/06/17\n",
        "Description: Play Atari Breakout with a Deep Q-Network.\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "## Introduction\n",
        "This script shows an implementation of Deep Q-Learning on the\n",
        "`BreakoutNoFrameskip-v4` environment.\n",
        "This example requires the following dependencies: `baselines`, `atari-py`, `rows`.\n",
        "They can be installed via:\n",
        "```\n",
        "git clone https://github.com/openai/baselines.git\n",
        "cd baselines\n",
        "pip install -e .\n",
        "git clone https://github.com/openai/atari-py\n",
        "wget http://www.atarimania.com/roms/Roms.rar\n",
        "unrar x Roms.rar .\n",
        "python -m atari_py.import_roms .\n",
        "```\n",
        "### Deep Q-Learning\n",
        "As an agent takes actions and moves through an environment, it learns to map\n",
        "the observed state of the environment to an action. An agent will choose an action\n",
        "in a given state based on a \"Q-value\", which is a weighted reward based on the\n",
        "expected highest long-term reward. A Q-Learning Agent learns to perform its\n",
        "task such that the recommended action maximizes the potential future rewards.\n",
        "This method is considered an \"Off-Policy\" method,\n",
        "meaning its Q values are updated assuming that the best action was chosen, even\n",
        "if the best action was not chosen.\n",
        "### Atari Breakout\n",
        "In this environment, a board moves along the bottom of the screen returning a ball that\n",
        "will destroy blocks at the top of the screen.\n",
        "The aim of the game is to remove all blocks and breakout of the\n",
        "level. The agent must learn to control the board by moving left and right, returning the\n",
        "ball and removing all the blocks without the ball passing the board.\n",
        "### Note\n",
        "The Deepmind paper trained for \"a total of 50 million frames (that is, around 38 days of\n",
        "game experience in total)\". However this script will give good results at around 10\n",
        "million frames which are processed in less than 24 hours on a modern machine.\n",
        "### References\n",
        "- [Q-Learning](https://link.springer.com/content/pdf/10.1007/BF00992698.pdf)\n",
        "- [Deep Q-Learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "## Setup\n",
        "\"\"\"\n",
        "\n",
        "from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Configuration paramaters for the whole setup\n",
        "seed = 42\n",
        "gamma = 0.99  # Discount factor for past rewards\n",
        "epsilon = 1.0  # Epsilon greedy parameter\n",
        "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
        "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
        "epsilon_interval = (\n",
        "    epsilon_max - epsilon_min\n",
        ")  # Rate at which to reduce chance of random action being taken\n",
        "batch_size = 32  # Size of batch taken from replay buffer\n",
        "max_steps_per_episode = 10000\n",
        "\n",
        "# Use the Baseline Atari environment because of Deepmind helper functions\n",
        "env = make_atari(\"BreakoutNoFrameskip-v4\")\n",
        "# Warp the frames, grey scale, stake four frame and scale to smaller ratio\n",
        "env = wrap_deepmind(env, frame_stack=True, scale=True)\n",
        "env.seed(seed)\n",
        "\n",
        "\"\"\"\n",
        "## Implement the Deep Q-Network\n",
        "This network learns an approximation of the Q-table, which is a mapping between\n",
        "the states and actions that an agent will take. For every state we'll have four\n",
        "actions, that can be taken. The environment provides the state, and the action\n",
        "is chosen by selecting the larger of the four Q-values predicted in the output layer.\n",
        "\"\"\"\n",
        "\n",
        "num_actions = 4\n",
        "\n",
        "\n",
        "def create_q_model():\n",
        "    # Network defined by the Deepmind paper\n",
        "    inputs = layers.Input(shape=(84, 84, 4,))\n",
        "\n",
        "    # Convolutions on the frames on the screen\n",
        "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
        "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
        "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
        "\n",
        "    layer4 = layers.Flatten()(layer3)\n",
        "\n",
        "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
        "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=action)\n",
        "\n",
        "\n",
        "# The first model makes the predictions for Q-values which are used to\n",
        "# make a action.\n",
        "model = create_q_model()\n",
        "# Build a target model for the prediction of future rewards.\n",
        "# The weights of a target model get updated every 10000 steps thus when the\n",
        "# loss between the Q-values is calculated the target Q-value is stable.\n",
        "model_target = create_q_model()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## Train\n",
        "\"\"\"\n",
        "# In the Deepmind paper they use RMSProp however then Adam optimizer\n",
        "# improves training time\n",
        "#optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
        "# improvement 1 optimizer using lr and df\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001,rho=gamma)\n",
        "\n",
        "# Experience replay buffers\n",
        "action_history = []\n",
        "state_history = []\n",
        "state_next_history = []\n",
        "rewards_history = []\n",
        "done_history = []\n",
        "episode_reward_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "frame_count = 0\n",
        "# improvement 2 remove epsilon_random_frames\n",
        "# Number of frames to take random action and observe output\n",
        "#epsilon_random_frames = 50000\n",
        "# Number of frames for exploration\n",
        "epsilon_greedy_frames = 1000000.0\n",
        "# Maximum replay length\n",
        "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
        "max_memory_length = 10000\n",
        "# Train the model after 4 actions\n",
        "update_after_actions = 4\n",
        "# How often to update the target network\n",
        "update_target_network = 10000\n",
        "# Using huber loss for stability\n",
        "loss_function = keras.losses.Huber()\n",
        "\n",
        "# while True:  # Run until solved\n",
        "# improvement 4 training is interrupted after 2000000 frames has been observed\n",
        "\n",
        "while frame_count <= 1000000:  \n",
        "    \n",
        "    state = np.array(env.reset())\n",
        "    episode_reward = 0\n",
        "\n",
        "    for timestep in range(1, max_steps_per_episode):\n",
        "        # env.render(); Adding this line would show the attempts\n",
        "        # of the agent in a pop up window.\n",
        "        frame_count += 1\n",
        "\n",
        "        # Use epsilon-greedy for exploration # changed to greedy \n",
        "        if epsilon > np.random.rand(1)[0]:\n",
        "            # Take random action\n",
        "            action = np.random.choice(num_actions)\n",
        "        else:\n",
        "            # Predict action Q-values\n",
        "            # From environment state\n",
        "            state_tensor = tf.convert_to_tensor(state)\n",
        "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
        "            action_probs = model(state_tensor, training=False)\n",
        "            # Take best action\n",
        "            action = tf.argmax(action_probs[0]).numpy()\n",
        "\n",
        "        # Decay probability of taking random action\n",
        "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
        "        epsilon = max(epsilon, epsilon_min)\n",
        "\n",
        "        # Apply the sampled action in our environment\n",
        "        state_next, reward, done, _ = env.step(action)\n",
        "        state_next = np.array(state_next)\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Save actions and states in replay buffer\n",
        "        action_history.append(action)\n",
        "        state_history.append(state)\n",
        "        state_next_history.append(state_next)\n",
        "        done_history.append(done)\n",
        "        rewards_history.append(reward)\n",
        "        state = state_next\n",
        "        # improvement 5 draw a batch and update only after the replay buffer is full\n",
        "        # Update every fourth frame and once batch size is over 32\n",
        "        if frame_count % update_after_actions == 0 and len(done_history) > max_memory_length:\n",
        "\n",
        "            # Get indices of samples for replay buffers\n",
        "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
        "\n",
        "            # Using list comprehension to sample from replay buffer\n",
        "            state_sample = np.array([state_history[i] for i in indices])\n",
        "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
        "            rewards_sample = [rewards_history[i] for i in indices]\n",
        "            action_sample = [action_history[i] for i in indices]\n",
        "            done_sample = tf.convert_to_tensor(\n",
        "                [float(done_history[i]) for i in indices]\n",
        "            )\n",
        "\n",
        "            # Build the updated Q-values for the sampled future states\n",
        "            # Use the target model for stability\n",
        "            future_rewards = model_target.predict(state_next_sample)\n",
        "            # Q value = reward + discount factor * expected future reward\n",
        "            #updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
        "                #future_rewards, axis=1\n",
        "            #)\n",
        "\n",
        "            # If final frame set the last value to -1\n",
        "            #updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
        "\n",
        "            #improvement 7 - correct implementation\n",
        "            updated_q_values = rewards_sample + (1- done_sample) * gamma * tf.reduce_max(future_rewards,axis=1)\n",
        "\n",
        "        \n",
        "\n",
        "            # Create a mask so we only calculate loss on the updated Q-values\n",
        "            masks = tf.one_hot(action_sample, num_actions)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Train the model on the states and updated Q-values\n",
        "                q_values = model(state_sample)\n",
        "\n",
        "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
        "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "                # Calculate loss between new Q-value and old Q-value\n",
        "                loss = loss_function(updated_q_values, q_action)\n",
        "\n",
        "            # Backpropagation\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        if frame_count % update_target_network == 0:\n",
        "            # update the the target network with new weights\n",
        "            model_target.set_weights(model.get_weights())\n",
        "            # Log details\n",
        "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
        "            print(template.format(running_reward, episode_count, frame_count))\n",
        "\n",
        "        # Limit the state and reward history\n",
        "        if len(rewards_history) > max_memory_length:\n",
        "            del rewards_history[:1]\n",
        "            del state_history[:1]\n",
        "            del state_next_history[:1]\n",
        "            del action_history[:1]\n",
        "            del done_history[:1]\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # improvement 6\n",
        "    running_reward_history = []\n",
        "    # Update running reward to check condition for solving\n",
        "    episode_reward_history.append(episode_reward)\n",
        "    \n",
        "    running_reward = np.mean(episode_reward_history[-100:])\n",
        "\n",
        "    episode_count += 1\n",
        "\n",
        "    if running_reward > 40:  # Condition to consider the task solved\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\n",
        "        break\n",
        "\n",
        "model.save('/content/drive/MyDrive/game_ai/model')\n",
        "np.save('/content/drive/MyDrive/game_ai/episode_reward_history.npy',episode_reward_history)\n",
        "\"\"\"\n",
        "## Visualizations\n",
        "Before any training:\n",
        "![Imgur](https://i.imgur.com/rRxXF4H.gif)\n",
        "In early stages of training:\n",
        "![Imgur](https://i.imgur.com/X8ghdpL.gif)\n",
        "In later stages of training:\n",
        "![Imgur](https://i.imgur.com/Z1K6qBQ.gif)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running reward: 0.32 at episode 306, frame count 10000\n",
            "running reward: 0.23 at episode 592, frame count 20000\n",
            "running reward: 0.37 at episode 861, frame count 30000\n",
            "running reward: 0.22 at episode 1155, frame count 40000\n",
            "running reward: 0.25 at episode 1470, frame count 50000\n",
            "running reward: 0.27 at episode 1766, frame count 60000\n",
            "running reward: 0.25 at episode 2050, frame count 70000\n",
            "running reward: 0.30 at episode 2330, frame count 80000\n",
            "running reward: 0.28 at episode 2610, frame count 90000\n",
            "running reward: 0.23 at episode 2904, frame count 100000\n",
            "running reward: 0.12 at episode 3226, frame count 110000\n",
            "running reward: 0.32 at episode 3529, frame count 120000\n",
            "running reward: 0.25 at episode 3844, frame count 130000\n",
            "running reward: 0.21 at episode 4152, frame count 140000\n",
            "running reward: 0.23 at episode 4476, frame count 150000\n",
            "running reward: 0.25 at episode 4777, frame count 160000\n",
            "running reward: 0.18 at episode 5072, frame count 170000\n",
            "running reward: 0.28 at episode 5343, frame count 180000\n",
            "running reward: 0.31 at episode 5630, frame count 190000\n",
            "running reward: 0.33 at episode 5925, frame count 200000\n",
            "running reward: 0.36 at episode 6192, frame count 210000\n",
            "running reward: 0.37 at episode 6435, frame count 220000\n",
            "running reward: 0.33 at episode 6692, frame count 230000\n",
            "running reward: 0.44 at episode 6935, frame count 240000\n",
            "running reward: 0.46 at episode 7183, frame count 250000\n",
            "running reward: 0.63 at episode 7395, frame count 260000\n",
            "running reward: 0.59 at episode 7613, frame count 270000\n",
            "running reward: 0.51 at episode 7845, frame count 280000\n",
            "running reward: 0.70 at episode 8046, frame count 290000\n",
            "running reward: 0.87 at episode 8225, frame count 300000\n",
            "running reward: 0.81 at episode 8411, frame count 310000\n",
            "running reward: 0.79 at episode 8593, frame count 320000\n",
            "running reward: 0.65 at episode 8784, frame count 330000\n",
            "running reward: 0.69 at episode 8978, frame count 340000\n",
            "running reward: 1.01 at episode 9142, frame count 350000\n",
            "running reward: 0.82 at episode 9325, frame count 360000\n",
            "running reward: 0.80 at episode 9503, frame count 370000\n",
            "running reward: 1.17 at episode 9664, frame count 380000\n",
            "running reward: 1.06 at episode 9817, frame count 390000\n",
            "running reward: 0.63 at episode 10002, frame count 400000\n",
            "running reward: 1.04 at episode 10157, frame count 410000\n",
            "running reward: 0.99 at episode 10318, frame count 420000\n",
            "running reward: 0.88 at episode 10497, frame count 430000\n",
            "running reward: 1.04 at episode 10666, frame count 440000\n",
            "running reward: 0.87 at episode 10838, frame count 450000\n",
            "running reward: 1.05 at episode 10996, frame count 460000\n",
            "running reward: 1.32 at episode 11135, frame count 470000\n",
            "running reward: 1.27 at episode 11286, frame count 480000\n",
            "running reward: 1.14 at episode 11431, frame count 490000\n",
            "running reward: 1.20 at episode 11576, frame count 500000\n",
            "running reward: 1.33 at episode 11713, frame count 510000\n",
            "running reward: 1.28 at episode 11850, frame count 520000\n",
            "running reward: 1.43 at episode 11984, frame count 530000\n",
            "running reward: 1.51 at episode 12116, frame count 540000\n",
            "running reward: 1.37 at episode 12252, frame count 550000\n",
            "running reward: 1.52 at episode 12375, frame count 560000\n",
            "running reward: 1.02 at episode 12518, frame count 570000\n",
            "running reward: 1.71 at episode 12631, frame count 580000\n",
            "running reward: 1.35 at episode 12766, frame count 590000\n",
            "running reward: 1.95 at episode 12869, frame count 600000\n",
            "running reward: 1.77 at episode 12983, frame count 610000\n",
            "running reward: 1.58 at episode 13110, frame count 620000\n",
            "running reward: 1.83 at episode 13220, frame count 630000\n",
            "running reward: 1.96 at episode 13324, frame count 640000\n",
            "running reward: 1.85 at episode 13436, frame count 650000\n",
            "running reward: 2.30 at episode 13531, frame count 660000\n",
            "running reward: 1.84 at episode 13647, frame count 670000\n",
            "running reward: 2.09 at episode 13754, frame count 680000\n",
            "running reward: 2.29 at episode 13845, frame count 690000\n",
            "running reward: 2.37 at episode 13939, frame count 700000\n",
            "running reward: 2.17 at episode 14040, frame count 710000\n",
            "running reward: 2.27 at episode 14139, frame count 720000\n",
            "running reward: 2.81 at episode 14225, frame count 730000\n",
            "running reward: 2.21 at episode 14328, frame count 740000\n",
            "running reward: 2.87 at episode 14403, frame count 750000\n",
            "running reward: 2.44 at episode 14495, frame count 760000\n",
            "running reward: 3.06 at episode 14570, frame count 770000\n",
            "running reward: 3.40 at episode 14638, frame count 780000\n",
            "running reward: 3.28 at episode 14723, frame count 790000\n",
            "running reward: 3.26 at episode 14788, frame count 800000\n",
            "running reward: 3.29 at episode 14861, frame count 810000\n",
            "running reward: 3.37 at episode 14927, frame count 820000\n",
            "running reward: 3.59 at episode 14991, frame count 830000\n",
            "running reward: 3.55 at episode 15059, frame count 840000\n",
            "running reward: 3.28 at episode 15130, frame count 850000\n",
            "running reward: 3.81 at episode 15191, frame count 860000\n",
            "running reward: 3.24 at episode 15268, frame count 870000\n",
            "running reward: 3.85 at episode 15328, frame count 880000\n",
            "running reward: 3.75 at episode 15393, frame count 890000\n",
            "running reward: 3.05 at episode 15470, frame count 900000\n",
            "running reward: 3.49 at episode 15532, frame count 910000\n",
            "running reward: 4.61 at episode 15586, frame count 920000\n",
            "running reward: 4.98 at episode 15640, frame count 930000\n",
            "running reward: 5.28 at episode 15686, frame count 940000\n",
            "running reward: 5.51 at episode 15727, frame count 950000\n",
            "running reward: 5.93 at episode 15776, frame count 960000\n",
            "running reward: 5.91 at episode 15822, frame count 970000\n",
            "running reward: 6.53 at episode 15860, frame count 980000\n",
            "running reward: 6.40 at episode 15907, frame count 990000\n",
            "running reward: 6.83 at episode 15947, frame count 1000000\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/game_ai/model/assets\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n## Visualizations\\nBefore any training:\\n![Imgur](https://i.imgur.com/rRxXF4H.gif)\\nIn early stages of training:\\n![Imgur](https://i.imgur.com/X8ghdpL.gif)\\nIn later stages of training:\\n![Imgur](https://i.imgur.com/Z1K6qBQ.gif)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import gym\n",
        "\n",
        "seed = 41\n",
        "\n",
        "model = keras.models.load_model('/content/drive/MyDrive/game_ai/model', compile=False)\n",
        "env = make_atari(\"BreakoutNoFrameskip-v4\")\n",
        "env = wrap_deepmind(env,frame_stack=True,scale=True)\n",
        "env.seed(seed)\n",
        "\n",
        "env = gym.wrappers.Monitor(env, '/content/drive/MyDrive/game_ai/videos', video_callable=lambda episode_id: True, force=True)\n",
        "\n",
        "n_episodes = 10\n",
        "returns = []\n",
        "\n",
        "for _ in range(n_episodes):\n",
        "    ret = 0\n",
        "\n",
        "    state = np.array(env.reset())\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "        # Predict action Q-values\n",
        "        # From environment state\n",
        "        state_tensor = tf.convert_to_tensor(state)\n",
        "        state_tensor = tf.expand_dims(state_tensor, 0)\n",
        "        action_probs = model(state_tensor, training=False)\n",
        "        # Take best action\n",
        "        action = tf.argmax(action_probs[0]).numpy()\n",
        "\n",
        "        # Apply the sampled action in our environment\n",
        "        state_next, reward, done, _ = env.step(action)\n",
        "        state_next = np.array(state_next)\n",
        "        state = state_next\n",
        "        ret += reward\n",
        "\n",
        "    returns.append(ret)\n",
        "\n",
        "    env.close()\n",
        "    print('Returns: {}'.format(returns))\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "k9dQOWEeA-St",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "277655ac-542b-4e33-c91a-f32b56e8de75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Returns: [14.0]\n",
            "Returns: [14.0, 16.0]\n",
            "Returns: [14.0, 16.0, 8.0]\n",
            "Returns: [14.0, 16.0, 8.0, 4.0]\n",
            "Returns: [14.0, 16.0, 8.0, 4.0, 2.0]\n",
            "Returns: [14.0, 16.0, 8.0, 4.0, 2.0, 4.0]\n",
            "Returns: [14.0, 16.0, 8.0, 4.0, 2.0, 4.0, 17.0]\n",
            "Returns: [14.0, 16.0, 8.0, 4.0, 2.0, 4.0, 17.0, 15.0]\n",
            "Returns: [14.0, 16.0, 8.0, 4.0, 2.0, 4.0, 17.0, 15.0, 3.0]\n",
            "Returns: [14.0, 16.0, 8.0, 4.0, 2.0, 4.0, 17.0, 15.0, 3.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# https://stackoverflow.com/questions/13728392/moving-average-or-running-mean \n",
        "# was used to make the running mean plot\n",
        "episode_reward_history = np.load('/content/drive/MyDrive/game_ai/episode_reward_history.npy')\n",
        "\n",
        "\n",
        "modes = ['valid']\n",
        "for valid in modes:\n",
        "    plt.plot(np.convolve(episode_reward_history, np.ones(100)/100, mode=valid));\n",
        "plt.axis([-10, 251, -.1, 1.1]);\n",
        "plt.legend(modes, loc='upper center');\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t3MWaMsV0tu3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "1f0e1158-33a8-4d41-9ed9-b09d06e3dbc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}